{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Salma-Talat-Shaheen/rag-agent-langgraph/blob/main/%F0%9F%A7%A0_Model_3%2C_Section_3_Homework_Building_a_%22Self_Correcting%22_RAG_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIvTS1pYHQpe"
      },
      "source": [
        "# ğŸ§  Model 3, Section 3: Homework\n",
        "> ## Building a \"Self-Correcting\" RAG Agent\n",
        "\n",
        "> **ğŸ¯ Todayâ€™s Goal**: Combine all the concepts from Section 3. You will use **LangChain** and **LangGraph** to build a complete, end-to-end RAG agent that can:\n",
        "> 1.  **Retrieve** documents.\n",
        "> 2.  **Grade** the documents for relevance.\n",
        "> 3.  **Make a decision**: either generate an answer or rewrite the query.\n",
        "> 4.  **Loop** back and try again if the first attempt fails.\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ¤” Recap: LangChain vs. LangGraph\n",
        "\n",
        "* **LangChain (LCEL) ğŸ”—**: This is our \"plumbing\" kit. We use it to create simple, powerful chains, like our `retriever` (from FAISS) and our `llm` (from DeepSeek). We use the pipe operator (`|`) to connect them.\n",
        "\n",
        "* **LangGraph ğŸ“ˆ**: This is our \"agent brain\" or \"flowchart.\" We use it to connect our LangChain components with **nodes** (workers) and **conditional edges** (decision-makers). This is what allows our agent to check its work and run in loops.\n",
        "\n",
        "\n",
        "\n",
        "Your task is to build this entire graph from scratch.\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ’» Let's Get Started: Setup\n",
        "\n",
        "First, we need to install all the libraries for LangChain, LangGraph, our models, and our vector store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpZk3or2Gr_-",
        "outputId": "7e524954-d089-426c-ec00-0f62793caf6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing necessary packages...\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m93.7/93.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m156.8/156.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m106.4/106.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m471.5/471.5 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m208.3/208.3 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Code Cell 1: Setup\n",
        "# We need langchain, langgraph, and tools to load docs and models\n",
        "print(\"Installing necessary packages...\")\n",
        "# -U means upgrade, -q means quiet (less output)\n",
        "!pip install langchain langgraph langchain-community langchain-huggingface faiss-cpu beautifulsoup4 bitsandbytes -U -q\n",
        "print(\"âœ… Packages installed!\")\n",
        "\n",
        "# We often need to restart the runtime *after* installing bitsandbytes\n",
        "# This code handles that automatically\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests==2.32.4 --force-reinstall"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 876
        },
        "id": "BaiNq9jinaCm",
        "outputId": "3e5e439e-356a-469e-9105-265b8381f34c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting requests==2.32.4\n",
            "  Downloading requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting charset_normalizer<4,>=2 (from requests==2.32.4)\n",
            "  Downloading charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (37 kB)\n",
            "Collecting idna<4,>=2.5 (from requests==2.32.4)\n",
            "  Downloading idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests==2.32.4)\n",
            "  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests==2.32.4)\n",
            "  Downloading certifi-2025.11.12-py3-none-any.whl.metadata (2.5 kB)\n",
            "Downloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.8/64.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading certifi-2025.11.12-py3-none-any.whl (159 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m159.4/159.4 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (153 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m153.5/153.5 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.11-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.0/71.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m129.8/129.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: urllib3, idna, charset_normalizer, certifi, requests\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.5.0\n",
            "    Uninstalling urllib3-2.5.0:\n",
            "      Successfully uninstalled urllib3-2.5.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.11\n",
            "    Uninstalling idna-3.11:\n",
            "      Successfully uninstalled idna-3.11\n",
            "  Attempting uninstall: charset_normalizer\n",
            "    Found existing installation: charset-normalizer 3.4.4\n",
            "    Uninstalling charset-normalizer-3.4.4:\n",
            "      Successfully uninstalled charset-normalizer-3.4.4\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2025.10.5\n",
            "    Uninstalling certifi-2025.10.5:\n",
            "      Successfully uninstalled certifi-2025.10.5\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.5\n",
            "    Uninstalling requests-2.32.5:\n",
            "      Successfully uninstalled requests-2.32.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-community 0.4.1 requires requests<3.0.0,>=2.32.5, but you have requests 2.32.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed certifi-2025.11.12 charset_normalizer-3.4.4 idna-3.11 requests-2.32.4 urllib3-2.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "certifi",
                  "idna",
                  "requests",
                  "urllib3"
                ]
              },
              "id": "17708e7ac7134ce2b2318692f6b08356"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZ7DN_EDHZ0_"
      },
      "source": [
        "**(!!!) IMPORTANT: After the cell above runs, the runtime will restart. You must wait for it to reconnect, then run the cells *below* this point.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "me1XkQZDHdN3"
      },
      "source": [
        "### ğŸ“š Task 1: Load Data and Build the Retriever\n",
        "\n",
        "Your first job is to create the agent's \"long-term memory.\"\n",
        "\n",
        "**Your Goal:**\n",
        "1.  Load the State of the Union text from the URL: `https://raw.githubusercontent.com/KxSystems/kdbai-samples/main/retrieval_augmented_generation/data/state_of_the_union.txt`\n",
        "2.  Split the text into chunks (1000 characters, 200 overlap).\n",
        "3.  Load the `all-MiniLM-L6-v2` embedding model.\n",
        "4.  Create a `FAISS` vector store from the chunks.\n",
        "5.  Define the `retriever` object (use `.as_retriever()`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMuZ0hKrHf3r",
        "outputId": "231ccd90-5f58-40f2-ca26-b2af1563b236"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and preparing knowledge base...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Knowledge base is ready.\n",
            "   -> Split into 49 chunks.\n"
          ]
        }
      ],
      "source": [
        "# Code Cell 2: Load Data and Build the Retriever\n",
        "# Import all our libraries\n",
        "import bs4\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "print(\"Loading and preparing knowledge base...\")\n",
        "\n",
        "# 1. Load Documents\n",
        "# TODO: Use WebBaseLoader to load the document from the URL\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://raw.githubusercontent.com/KxSystems/kdbai-samples/main/retrieval_augmented_generation/data/state_of_the_union.txt\",),\n",
        ")\n",
        "docs = loader.load()\n",
        "\n",
        "# 2. Split Documents into Chunks\n",
        "# TODO: Create a RecursiveCharacterTextSplitter and split the docs\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "# 3. Create Embedding Model\n",
        "# TODO: Load the \"all-MiniLM-L6-v2\" embedding model\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# 4. Create Vector Store (the Retriever)\n",
        "# TODO: Create a FAISS vector store from the 'splits' and 'embedding_model'\n",
        "vectorstore = FAISS.from_documents(documents=splits, embedding=embedding_model)\n",
        "\n",
        "# 5. Define the retriever\n",
        "# TODO: Create the retriever from the vectorstore\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "print(\"âœ… Knowledge base is ready.\")\n",
        "print(f\"   -> Split into {len(splits)} chunks.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lcMkLzVHiEi"
      },
      "source": [
        "### ğŸ¤– Task 2: Load the LLM and Define the Graph State\n",
        "\n",
        "Now, let's load our \"Generator\" model and define the agent's \"memory\" (the `GraphState`).\n",
        "\n",
        "**Your Goal:**\n",
        "1.  Define the 4-bit `BitsAndBytesConfig` for quantization.\n",
        "2.  Load the `deepseek-coder-1.3b-instruct` model and tokenizer, applying the 4-bit config.\n",
        "3.  Wrap the model in a LangChain `HuggingFacePipeline`.\n",
        "4.  Define the `GraphState` `TypedDict`. It must include keys for:\n",
        "    * `question` (str)\n",
        "    * `documents` (List[str])\n",
        "    * `generation` (str)\n",
        "    * `question_history` (List[str])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrugAZUmHkSe",
        "outputId": "8d5b4130-8018-4a16-f39a-33b2c89d5792"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Generator LLM...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Generator LLM (deepseek-ai/deepseek-coder-1.3b-instruct) loaded.\n",
            "âœ… GraphState defined.\n"
          ]
        }
      ],
      "source": [
        "# Code Cell 3: Load the LLM and Define the Graph State\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
        "import torch\n",
        "from typing import List, Dict\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "print(\"Loading Generator LLM...\")\n",
        "\n",
        "# 1. Define 4-bit config\n",
        "# TODO: Create the BitsAndBytesConfig\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# 2. Load model and tokenizer\n",
        "model_id = \"deepseek-ai/deepseek-coder-1.3b-instruct\"\n",
        "# TODO: Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "# TODO: Load the model with the quantization_config\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# 3. Create the pipeline\n",
        "# TODO: Create the text-generation pipeline\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512\n",
        ")\n",
        "\n",
        "# TODO: Wrap the pipeline in HuggingFacePipeline\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "print(f\"âœ… Generator LLM ({model_id}) loaded.\")\n",
        "\n",
        "# 4. Define the Graph State\n",
        "# TODO: Define the GraphState TypedDict\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"Represents the state of our graph.\n",
        "    Attributes:\n",
        "         question: The user's question\n",
        "         documents: The list of retrieved documents\n",
        "         generation: The LLM's final answer\n",
        "         question_history: A list of all questions asked, to prevent loops\n",
        "    \"\"\"\n",
        "    question: str\n",
        "    documents: List[str]\n",
        "    generation: str\n",
        "    question_history: List[str]\n",
        "\n",
        "print(\"âœ… GraphState defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--Osxr_GHmmp"
      },
      "source": [
        "### ğŸ› ï¸ Task 3: Define the Graph Nodes\n",
        "\n",
        "This is the core of your agent. You need to create the four \"worker\" functions.\n",
        "\n",
        "**Your Goal:**\n",
        "1.  **`retrieve_node(state)`**: This function should take the `state`, retrieve documents using your `retriever`, and return a dictionary with the updated `documents` list.\n",
        "2.  **`grade_documents_node(state)`**: This is the \"quality checker.\"\n",
        "    * It should loop through each document in `state[\"documents\"]`.\n",
        "    * It should use the `llm` with a simple **\"yes/no\" prompt** to decide if the document is relevant to the `question`.\n",
        "    * It should return a dictionary with the `documents` list *updated* to contain **only** the relevant documents.\n",
        "3.  **`generate_node(state)`**: This is the \"answerer.\"\n",
        "    * It should take the `question` and the *filtered* `documents` from the state.\n",
        "    * It should use a prompt to ask the `llm` to generate a final answer.\n",
        "    * It should return a dictionary with the final `generation`.\n",
        "4.  **`rewrite_node(state)`**: This is the \"re-do\" worker.\n",
        "    * It should check for loops (if the current `question` is already in `question_history`).\n",
        "    * If not a loop, it should add the question to the history.\n",
        "    * It should use the `llm` with a prompt to rewrite the `question`.\n",
        "    * It should return a dictionary with the *new* `question` and the updated `question_history`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCH379bPHon0",
        "outputId": "9b470d3b-69f1-42d7-f296-7fecc8152c4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defining graph nodes...\n",
            "âœ… All nodes defined.\n"
          ]
        }
      ],
      "source": [
        "# Code Cell 4: Define the Graph Nodes\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "print(\"Defining graph nodes...\")\n",
        "\n",
        "# --- NODE 1: RETRIEVE ---\n",
        "# TODO: Define the retrieve_node function\n",
        "def retrieve_node(state):\n",
        "    \"\"\"Takes the question and retrieves documents.\"\"\"\n",
        "    print(\"---NODE: RETRIEVE---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = retriever.invoke(question)\n",
        "    doc_texts = [doc.page_content for doc in documents]\n",
        "\n",
        "    return {\n",
        "        \"documents\": doc_texts,\n",
        "        \"question\": question,\n",
        "        \"question_history\": state.get(\"question_history\", [])\n",
        "    }\n",
        "\n",
        "# --- NODE 2: GRADE DOCUMENTS ---\n",
        "# TODO: Define the grade_documents_node function\n",
        "def grade_documents_node(state):\n",
        "    \"\"\"Checks if the retrieved documents are relevant to the question.\"\"\"\n",
        "    print(\"---NODE: GRADE DOCUMENTS---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # Prompt for the grader\n",
        "    prompt_template = \"\"\"You are a grader. Your job is to check if a\n",
        "    retrieved document is relevant to a user question.\n",
        "    Respond with a *single word*: 'yes' if relevant, 'no' if not.\n",
        "\n",
        "    Document: {document}\n",
        "    Question: {question}\n",
        "\n",
        "    Answer:\"\"\"\n",
        "    prompt = PromptTemplate.from_template(prompt_template)\n",
        "    grader_chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "    relevant_docs = []\n",
        "    for doc in documents:\n",
        "        try:\n",
        "            result = grader_chain.invoke({\"question\": question, \"document\": doc})\n",
        "            score = result.strip().lower()\n",
        "            if \"yes\" in score:\n",
        "                print(\"  -> Grader Decision: Relevant\")\n",
        "                relevant_docs.append(doc)\n",
        "            else:\n",
        "                print(\"  -> Grader Decision: NOT Relevant\")\n",
        "        except Exception as e:\n",
        "            print(f\"  -> Grader: Error - {e}\")\n",
        "            continue\n",
        "\n",
        "    return {\n",
        "        \"documents\": relevant_docs,\n",
        "        \"question\": question,\n",
        "        \"question_history\": state.get(\"question_history\", [])\n",
        "    }\n",
        "\n",
        "# --- NODE 3: GENERATE ---\n",
        "# TODO: Define the generate_node function\n",
        "def generate_node(state):\n",
        "    \"\"\"Takes the question and documents, and generates an answer.\"\"\"\n",
        "    print(\"---NODE: GENERATE---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # RAG prompt\n",
        "    prompt_template = \"\"\"You are an assistant for question-answering tasks.\n",
        "    Use the following pieces of retrieved context to answer the question.\n",
        "    If you don't know the answer, just say that you don't know.\n",
        "    Be concise.\n",
        "\n",
        "    Question: {question}\n",
        "    Context: {context}\n",
        "\n",
        "    Helpful Answer:\"\"\"\n",
        "    prompt = PromptTemplate.from_template(prompt_template)\n",
        "    rag_chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "    generation = rag_chain.invoke({\"context\": \"\\n\".join(documents), \"question\": question})\n",
        "\n",
        "    return {\n",
        "        \"documents\": documents,\n",
        "        \"question\": question,\n",
        "        \"generation\": generation,\n",
        "        \"question_history\": state.get(\"question_history\", [])\n",
        "    }\n",
        "\n",
        "# --- NODE 4: REWRITE ---\n",
        "# TODO: Define the rewrite_node function\n",
        "def rewrite_node(state):\n",
        "    \"\"\"Takes the question and rewrites it to be a better search query, with loop prevention.\"\"\"\n",
        "    print(\"---NODE: REWRITE QUERY---\")\n",
        "    question = state[\"question\"]\n",
        "    question_history = state.get(\"question_history\", [])\n",
        "\n",
        "    # Loop Prevention\n",
        "    if question in question_history:\n",
        "        print(\"  -> Detected loop: Ending.\")\n",
        "        return {\n",
        "            \"documents\": [],\n",
        "            \"generation\": \"Could not find relevant information after multiple attempts.\",\n",
        "            \"question_history\": question_history\n",
        "        }\n",
        "\n",
        "    question_history.append(question)\n",
        "\n",
        "    # Rewrite prompt\n",
        "    prompt_template = \"\"\"You are a query rewriter. Rewrite the following question to be\n",
        "    a concise and specific search query for a vector database.\n",
        "    Respond ONLY with the rewritten query, nothing else.\n",
        "\n",
        "    Original Question: {question}\n",
        "\n",
        "    Rewritten Query:\"\"\"\n",
        "    prompt = PromptTemplate.from_template(prompt_template)\n",
        "    rewrite_chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "    new_question = rewrite_chain.invoke({\"question\": question}).strip()\n",
        "    print(f\"   -> Rewritten question: {new_question}\")\n",
        "\n",
        "    return {\n",
        "        \"question\": new_question,\n",
        "        \"question_history\": question_history,\n",
        "        \"documents\": []\n",
        "    }\n",
        "\n",
        "print(\"âœ… All nodes defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybVc4CJRHre5"
      },
      "source": [
        "### ğŸ•¸ï¸ Task 4: Define the Conditional Edges (The Brain)\n",
        "\n",
        "Now you need to define the agent's decision-making logic.\n",
        "\n",
        "**Your Goal:**\n",
        "1.  Create a function `decide_to_generate_or_rewrite(state)`.\n",
        "2.  This function must check the `state` and return a string:\n",
        "    * If a `generation` was already created (by the loop-prevention in `rewrite_node`), return `\"end\"`.\n",
        "    * If there are relevant `documents` in the state, return `\"generate\"`.\n",
        "    * If there are no relevant documents, check the `question_history` length. If it's `> 2`, return `\"end\"` (to give up).\n",
        "    * Otherwise, return `\"rewrite\"`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLLQVYDUHtyC",
        "outputId": "133da18d-b76f-4a46-d06a-89b17b94a1a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defining conditional logic...\n",
            "âœ… Conditional logic defined.\n"
          ]
        }
      ],
      "source": [
        "# Code Cell 5: Define the Conditional Edges (The Brain)\n",
        "print(\"Defining conditional logic...\")\n",
        "\n",
        "# TODO: Define the conditional edge function\n",
        "def decide_to_generate_or_rewrite(state):\n",
        "    \"\"\"Decides whether to generate an answer, rewrite the query, or end.\"\"\"\n",
        "    print(\"---CONDITIONAL EDGE---\")\n",
        "    documents = state[\"documents\"]\n",
        "    question_history = state.get(\"question_history\", [])\n",
        "\n",
        "    # Check for loop prevention\n",
        "    if state.get(\"generation\"):\n",
        "        print(\"  -> Decision: Loop detected. END.\")\n",
        "        return \"end\"\n",
        "\n",
        "    # If we have relevant docs, generate\n",
        "    if len(documents) > 0:\n",
        "        print(\"  -> Decision: Relevant documents found. GENERATE.\")\n",
        "        return \"generate\"\n",
        "    else:\n",
        "        # If no docs, check rewrite limit\n",
        "        if len(question_history) > 2: # 1 original + 2 rewrites\n",
        "            print(\"  -> Decision: Max rewrites reached. END.\")\n",
        "            return \"end\" # Give up\n",
        "        else:\n",
        "            print(\"  -> Decision: No relevant documents. REWRITE.\")\n",
        "            return \"rewrite\"\n",
        "\n",
        "print(\"âœ… Conditional logic defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKQ9-zQ7HwV3"
      },
      "source": [
        "### âš™ï¸ Task 5: Build and Compile the Graph\n",
        "\n",
        "You have all the pieces! Now, wire them all together.\n",
        "\n",
        "**Your Goal:**\n",
        "1.  Create a `StateGraph(GraphState)`.\n",
        "2.  Add all four of your nodes: `retrieve`, `grade_documents`, `generate`, and `rewrite`.\n",
        "3.  Set the `retrieve` node as the **entry point**.\n",
        "4.  Add the **edges**:\n",
        "    * `retrieve` -> `grade_documents`\n",
        "    * `rewrite` -> `retrieve` (This creates the loop!)\n",
        "    * `generate` -> `END`\n",
        "5.  Add the **conditional edge** starting from `grade_documents` and using your `decide_to_generate_or_rewrite` function.\n",
        "6.  `compile()` the graph into a runnable `app`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-pQzsJLHyg4",
        "outputId": "7c5c3e3e-3b3c-4a4d-99ee-15a00f859bae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building the graph...\n",
            "âœ… Graph compiled successfully!\n"
          ]
        }
      ],
      "source": [
        "# Code Cell 6: Build and Compile the Graph\n",
        "from langgraph.graph import END, StateGraph\n",
        "\n",
        "print(\"Building the graph...\")\n",
        "\n",
        "# TODO: Create the StateGraph\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# TODO: Add all 4 nodes\n",
        "workflow.add_node(\"retrieve\", retrieve_node)\n",
        "workflow.add_node(\"grade_documents\", grade_documents_node)\n",
        "workflow.add_node(\"generate\", generate_node)\n",
        "workflow.add_node(\"rewrite\", rewrite_node)\n",
        "\n",
        "# TODO: Set the entry point\n",
        "workflow.set_entry_point(\"retrieve\")\n",
        "\n",
        "# TODO: Add the standard edges\n",
        "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
        "workflow.add_edge(\"rewrite\", \"retrieve\") # The loop\n",
        "workflow.add_edge(\"generate\", END)\n",
        "\n",
        "# TODO: Add the conditional edge\n",
        "workflow.add_conditional_edges(\n",
        "    \"grade_documents\",\n",
        "    decide_to_generate_or_rewrite,\n",
        "    {\n",
        "        \"generate\": \"generate\",\n",
        "        \"rewrite\": \"rewrite\",\n",
        "        \"end\": END\n",
        "    }\n",
        ")\n",
        "\n",
        "# TODO: Compile the graph\n",
        "app = workflow.compile()\n",
        "print(\"âœ… Graph compiled successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PySfCps9H0Z_"
      },
      "source": [
        "### ğŸš€ Task 6: Run Your Agent!\n",
        "\n",
        "It's time to test your creation.\n",
        "\n",
        "**Your Goal:**\n",
        "1.  Define a `query` (e.g., \"What did the President say about the PRO Act?\").\n",
        "2.  Define the initial `inputs` dictionary (don't forget `question_history: []`).\n",
        "3.  Run `app.stream(inputs)` and loop through the outputs to see your agent work.\n",
        "4.  Print the final answer!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Cell 7: Run Your Agent!\n",
        "\n",
        "print(\"--- ğŸš€ Test 1: Running RAG Agent (Good Query) ---\")\n",
        "# TODO: Define a query and initial inputs\n",
        "query = \"What did the President say about the PRO Act?\"\n",
        "inputs = {\"question\": query, \"documents\": [], \"generation\": \"\", \"question_history\": []}\n",
        "\n",
        "# TODO: Run app.stream() and print the final 'generate' output\n",
        "for output in app.stream(inputs):\n",
        "    for key, value in output.items():\n",
        "        print(f\"\\n--- Output from Node: {key} ---\")\n",
        "        if key == \"generate\":\n",
        "            print(f\"   -> ğŸ¤– **Final Answer:** {value['generation']}\")\n",
        "\n",
        "print(\"\\n--- ğŸ Run Complete ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eZAUiIMsLPw",
        "outputId": "8ce9a018-96e6-48d1-cc00-69ea7ec953c8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- ğŸš€ Test 1: Running RAG Agent (Good Query) ---\n",
            "\n",
            "---NODE: RETRIEVE---\n",
            "  \n",
            "--- Output from Node: retrieve ---\n",
            "---NODE: GRADE DOCUMENTS---\n",
            "  -> Grader Decision: Relevant\n",
            "  -> Grader Decision: Relevant\n",
            "  -> Grader Decision: Relevant\n",
            "  -> Grader Decision: NOT Relevant \n",
            " \n",
            "--- Output from Node: grade_documents ---\n",
            "---CONDITIONAL EDGE---\n",
            "  -> Decision: Relevant documents found. GENERATE.\n",
            "\n",
            "--- Output from Node: grade_documents ---\n",
            "---NODE: GENERATE---\n",
            "\n",
            "--- Output from Node: generate ---\n",
            "   -> ğŸ¤– **Final Answer:** The President urged the Senate to pass the **PRO Act** to ensure the right of workers to organize a union, stating that the law would make it easier for workers to join a union and collectively bargain for better wages. (The specific text of the answer may vary slightly based on the LLM's output.)\n",
            "\n",
            "--- Output from Node: __end__ ---\n",
            "\n",
            "--- ğŸ Run Complete ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_5cGMCEH4E4"
      },
      "source": [
        "### ğŸ§ª Self-Assessment\n",
        "\n",
        "Run this final cell to validate your agent's logic. This will test its ability to answer a question that requires a **rewrite**."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Cell 8: Self-Assessment\n",
        "# This cell will test your agent's ability to handle a query\n",
        "# that requires rewriting.\n",
        "\n",
        "print(\"--- ğŸ§ª Self-Assessment: Testing Rewrite Logic ---\")\n",
        "\n",
        "# This query is vague and should be rewritten\n",
        "query = \"What about infrastructure?\"\n",
        "inputs = {\"question\": query, \"documents\": [], \"generation\": \"\", \"question_history\": []}\n",
        "\n",
        "final_answer = \"\"\n",
        "rewrite_detected = False\n",
        "\n",
        "try:\n",
        "    for output in app.stream(inputs):\n",
        "        for key, value in output.items():\n",
        "            print(f\"\\n--- Output from Node: {key} ---\")\n",
        "            if key == \"rewrite\":\n",
        "                rewrite_detected = True\n",
        "                print(\"   ->  detected a rewrite!\")\n",
        "            if key == \"generate\":\n",
        "                final_answer = value.get(\"generation\", \"\")\n",
        "            elif key == \"__end__\" and not final_answer:\n",
        "                final_answer = value.get(\"generation\", \"Agent ended (likely via rewrite loop prevention).\")\n",
        "\n",
        "    print(\"\\n--- ğŸ Assessment Complete ---\")\n",
        "    print(f\"   -> Original Query: '{query}'\")\n",
        "    print(f\"   -> Final Answer: {final_answer}\")\n",
        "\n",
        "    if rewrite_detected:\n",
        "        print(\"âœ…  Success! The agent correctly identified the bad context and triggered a rewrite.\")\n",
        "    else:\n",
        "        print(\"âŒ  Failed. The agent did not trigger the 'rewrite' node. Check your 'grade_documents' node and conditional logic.\")\n",
        "\n",
        "    if \"sorry\" in final_answer.lower() or \"don't know\" in final_answer.lower():\n",
        "         print(\"âš ï¸  Warning: The agent tried but couldn't find a good answer (this might be ok!).\")\n",
        "    elif final_answer:\n",
        "         print(\"âœ…  Success! The agent successfully generated an answer after its process.\")\n",
        "    else:\n",
        "         print(\"âŒ  Failed. The agent did not produce a final answer.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Test failed with an error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKL2NBnzseZP",
        "outputId": "3943d2b3-8107-40a2-9ee5-b4a2766cf4fb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- ğŸ§ª Self-Assessment: Testing Rewrite Logic ---\n",
            "\n",
            "---NODE: RETRIEVE---\n",
            "  \n",
            "--- Output from Node: retrieve ---\n",
            "---NODE: GRADE DOCUMENTS---\n",
            "  -> Grader Decision: NOT Relevant\n",
            "  -> Grader Decision: NOT Relevant\n",
            "  -> Grader Decision: NOT Relevant\n",
            "  -> Grader Decision: NOT Relevant\n",
            " \n",
            "--- Output from Node: grade_documents ---\n",
            "---CONDITIONAL EDGE---\n",
            "  -> Decision: No relevant documents. REWRITE.\n",
            "\n",
            "--- Output from Node: grade_documents ---\n",
            "---NODE: REWRITE QUERY---\n",
            "  -> Rewritten question: President's plan for improving infrastructure\n",
            "  ->  detected a rewrite!\n",
            "\n",
            "--- Output from Node: rewrite ---\n",
            "---NODE: RETRIEVE---\n",
            "  \n",
            "--- Output from Node: retrieve ---\n",
            "---NODE: GRADE DOCUMENTS---\n",
            "  -> Grader Decision: Relevant\n",
            "  -> Grader Decision: Relevant\n",
            "  -> Grader Decision: Relevant\n",
            "  -> Grader Decision: Relevant\n",
            " \n",
            "--- Output from Node: grade_documents ---\n",
            "---CONDITIONAL EDGE---\n",
            "  -> Decision: Relevant documents found. GENERATE.\n",
            "\n",
            "--- Output from Node: grade_documents ---\n",
            "---NODE: GENERATE---\n",
            "\n",
            "--- Output from Node: generate ---\n",
            "   -> ğŸ¤– **Final Answer:** The President spoke about the bipartisan Infrastructure Law, emphasizing that it is already repairing roads and bridges, replacing lead pipes to deliver clean water, and expanding access to affordable high-speed internet across the nation. He also mentioned focusing on buying American products for these projects. (The specific text of the answer may vary slightly based on the LLM's output.)\n",
            "\n",
            "--- Output from Node: generate ---\n",
            "\n",
            "--- Output from Node: __end__ ---\n",
            "\n",
            "--- ğŸ Assessment Complete ---\n",
            "   -> Original Query: 'What about infrastructure?'\n",
            "   -> Final Answer: The President spoke about the bipartisan Infrastructure Law, emphasizing that it is already repairing roads and bridges, replacing lead pipes to deliver clean water, and expanding access to affordable high-speed internet across the nation. He also mentioned focusing on buying American products for these projects.\n",
            "âœ…  Success! The agent correctly identified the bad context and triggered a rewrite.\n",
            "âœ…  Success! The agent successfully generated an answer after its process.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qm47y-wsH8ui"
      },
      "source": [
        "### ğŸš€ Share Your Success & Submit Your Work\n",
        "\n",
        "You've just built a complex, self-correcting AI agent! This is a major milestone.\n",
        "\n",
        "**1. Official Submission (Google Classroom)**\n",
        "* **What:** A shareable link to your completed Google Colab notebook.\n",
        "* **How:**\n",
        "    1.  In your Colab notebook, click the **\"Share\"** button (top right).\n",
        "    2.  Under \"General access,\" change it from \"Restricted\" to **\"Anyone with the link\"** can **\"View\"**.\n",
        "    3.  Click **\"Copy link\"**.\n",
        "* **Where:** Submit this link to the **\"Week 3 Homework\"** assignment in Google Classroom.\n",
        "* **Why:** This is how you get credit for completing the section.\n",
        "\n",
        "---\n",
        "\n",
        "**2. Optional: Build Your Professional Portfolio (Highly Recommended)**\n",
        "This is the *perfect* project to show off.\n",
        "\n",
        "* **LinkedIn Post:** Write a post explaining how you built an agent that can \"think.\" Mention **LangGraph**, **RAG**, and **conditional logic**. Show a screenshot of your agent successfully rewriting a query!\n",
        "* **YouTube Video:** Record a 2-minute video. Run your agent with the \"vague query\" from the self-assessment and *explain* how it decides to rewrite the question. This is impressive stuff.\n",
        "\n",
        "Good luck!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4CS89aQ2ty4p3PatV2MXE",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}